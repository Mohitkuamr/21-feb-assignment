{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d773a1-a3ac-4927-9b60-55c6b6f118c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans = 1\n",
    "Web scraping, also known as web harvesting or data scraping, is the process of extracting data from websites\n",
    "automatically using software or programming tools. The data can include text, images, videos, links, and other\n",
    "types of content that are publicly available on the internet.\n",
    "\n",
    "Web scraping is used for various purposes, such as market research, competitor analysis, content aggregation,\n",
    "data mining, and more. It is particularly useful when large amounts of data need to be collected from multiple\n",
    "sources quickly and efficiently.\n",
    "\n",
    "1 = E-commerce\n",
    "2 = social media\n",
    "3 = Reserach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7ddf0e-e3b3-424b-8eda-0e283f8bf963",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans = 2\n",
    "\n",
    "There are several methods used for web scraping, and each method has its own advantages and disadvantages. Here are some of the most commonly used methods:\n",
    "\n",
    "1.\n",
    "Manual scraping: This involves manually copying and pasting data from a website into a spreadsheet or other file. \n",
    "This method is time-consuming and can be error-prone, but it can be useful for small-scale data collection.\n",
    "\n",
    "2.\n",
    "Web scraping software: This involves using software such as web crawlers, bots, or spiders to automatically\n",
    "extract data from websites. This method is faster and more efficient than manual scraping, but it requires\n",
    "some programming skills and may not work for all websites.\n",
    "\n",
    "3.\n",
    "Application programming interfaces (APIs): Some websites provide APIs that allow developers to access their \n",
    "data in a structured format. This method is the most reliable and efficient way to collect data, but it requires \n",
    "some knowledge of programming and may be subject to usage limits and fees.\n",
    "\n",
    "4.\n",
    "Parsing HTML: This involves using programming languages such as Python or JavaScript to parse the HTML code \n",
    "of a website and extract the desired data. This method is flexible and can work for any website, but it requires\n",
    "some programming skills and may be affected by changes to the website structure.\n",
    "\n",
    "5.\n",
    "Scraping as a service: This involves outsourcing web scraping to a third-party service provider that specializes\n",
    "in data extraction. This method can be convenient and cost-effective, but it requires finding a trustworthy\n",
    "provider and may be subject to usage limits and fees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65b5d53-2480-40e8-8aaa-ea69e9f33533",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans = 3\n",
    "Beautiful Soup is used because it makes it easy to navigate and search HTML and XML documents, and extract\n",
    "the specific data that is needed. It is particularly useful when the HTML code is complex or poorly structured,\n",
    "as it can handle imperfect markup and still extract the desired data.\n",
    "\n",
    "Some of the key features of Beautiful Soup include:\n",
    "\n",
    "1.\n",
    "Navigation: Beautiful Soup allows you to navigate the HTML or XML document using a variety of methods, such as \n",
    "searching by tag name, class, or attribute.\n",
    "\n",
    "3.\n",
    "Parsing: Beautiful Soup can parse HTML and XML documents and convert them into a parse tree, which can \n",
    "be searched and manipulated.\n",
    "\n",
    "4.\n",
    "Data extraction: Beautiful Soup allows you to extract specific data from the HTML or XML document, such as text, \n",
    "links, images, or tables.\n",
    "\n",
    "5.\n",
    "Encoding detection: Beautiful Soup can detect the encoding of the document and convert it to Unicode, which makes\n",
    "it easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a28d75-d7ec-47f3-8445-6e6a2aac40f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans = 4\n",
    "Here are some of the reasons why Flask may be used in a web scraping project:\n",
    "\n",
    "1.\n",
    "Easy to set up: Flask is a lightweight framework that is easy to install and set up, even for beginners.\n",
    "\n",
    "2.\n",
    "Flexible routing: Flask allows you to define routes and endpoints for handling different types of requests, \n",
    "such as GET and POST requests.\n",
    "\n",
    "3.\n",
    "Template rendering: Flask comes with a built-in template engine that allows you to render HTML templates\n",
    "and dynamically generate web pages.\n",
    "\n",
    "4.\n",
    "Integration with other libraries: Flask can be easily integrated with other Python libraries, such as Beautiful \n",
    "Soup for web scraping and Pandas for data manipulation.\n",
    "\n",
    "5.\n",
    "Deployment options: Flask applications can be easily deployed to a variety of hosting platforms, such as Heroku \n",
    "or AWS, which makes it easy to share your web scraping project with others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2ea136-9a9a-4947-84b9-fa6d44a4af24",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans = 5\n",
    "1.\n",
    "EC2 (Elastic Compute Cloud): EC2 is a virtual machine service that allows users to rent and configure virtual\n",
    "servers in the cloud. In a web scraping project, EC2 could be used to host the web scraping code and run it on \n",
    "a remote server. This would allow the code to run continuously, even if the user local machine is turned off or\n",
    "disconnected from the internet.\n",
    "\n",
    "2.\n",
    "S3 (Simple Storage Service): S3 is a cloud storage service that allows users to store and retrieve files and \n",
    "objects in the cloud. In a web scraping project, S3 could be used to store the scraped data in a scalable and\n",
    "durable storage solution that can be accessed from anywhere. This would allow multiple users to access the data \n",
    "and perform analysis on it without having to download it to their local machines.\n",
    "\n",
    "3.\n",
    "Lambda: Lambda is a serverless computing service that allows users to run code in response to events, without\n",
    "having to manage any infrastructure. In a web scraping project, Lambda could be used to trigger the scraping \n",
    "code in response to certain events, such as a new URL being added to a queue. This would allow the code to run \n",
    "automatically and scale up or down based on the demand.\n",
    "\n",
    "4.\n",
    "API Gateway: API Gateway is a service that allows users to create and manage APIs that can be used to access \n",
    "backend services. In a web scraping project, API Gateway could be used to expose the scraped data through \n",
    "a REST API, allowing users to access the data programmatically and integrate it with other applications.\n",
    "\n",
    "5.\n",
    "CloudWatch: CloudWatch is a monitoring and logging service that allows users to monitor their AWS resources and \n",
    "applications in real-time. In a web scraping project, CloudWatch could be used to monitor the performance and\n",
    "health of the web scraping code, and alert users if any issues arise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
